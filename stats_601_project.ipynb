{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashpatel5400/crypto-prediction/blob/main/stats_601_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "mNp93N3fBzaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "ccADqYamxwt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66507eb-7887-4bc1-e838-1661b39b3647"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('volu.csv', <http.client.HTTPMessage at 0x7f9b33e56510>)"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "import urllib.request\n",
        "from joblib import dump, load\n",
        "\n",
        "url = \"https://media.githubusercontent.com/media/yashpatel5400/crypto-prediction/main/log_pr.csv\"\n",
        "urllib.request.urlretrieve(url, \"log_pr.csv\")\n",
        "\n",
        "url = \"https://media.githubusercontent.com/media/yashpatel5400/crypto-prediction/main/volu.csv\"\n",
        "urllib.request.urlretrieve(url, \"volu.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_pr = pd.read_csv(\"log_pr.csv\", index_col= 0)\n",
        "volu = pd.read_csv(\"volu.csv\", index_col= 0)\n",
        "\n",
        "print(len(log_pr))\n",
        "\n",
        "log_pr.index = pd.to_datetime(log_pr.index)\n",
        "volu.index = pd.to_datetime(volu.index)\n",
        "\n",
        "in_sample_size = 50000\n",
        "out_sample_size = 25000\n",
        "set_size = in_sample_size + out_sample_size\n",
        "\n",
        "start_index = 0\n",
        "log_pr_kfolds = []\n",
        "volu_kfolds = []\n",
        "while start_index + set_size < len(log_pr):\n",
        "  end_index = start_index + set_size\n",
        "  log_prfold = log_pr.iloc[start_index:end_index, :] \n",
        "  volufold = volu.iloc[start_index:end_index, :] \n",
        "\n",
        "  log_pr_kfolds.append((log_prfold.iloc[:-out_sample_size, :], log_prfold.iloc[-out_sample_size:, :])) #(train, test)\n",
        "  volu_kfolds.append((volufold.iloc[:-out_sample_size, :], volufold.iloc[-out_sample_size:, :]))\n",
        "  start_index += out_sample_size\n",
        "\n",
        "print(len(log_pr_kfolds))\n",
        "\n",
        "train_size = 0\n",
        "out_sample_size = len(log_pr)-1\n",
        "log_pr_anchoredfolds = []\n",
        "volu_anchoredfolds = []\n",
        "while train_size +  out_sample_size < len(log_pr):\n",
        "  log_pr_anchoredfolds.append((log_pr.iloc[0:train_size, :] , log_pr.iloc[train_size:train_size+out_sample_size, :])) #(train, test)\n",
        "  volu_anchoredfolds.append((volu.iloc[0:train_size, :] , volu.iloc[train_size:train_size+out_sample_size, :])) #(train, test)\n",
        "  train_size += out_sample_size\n",
        "\n",
        "print(len(log_pr_anchoredfolds))"
      ],
      "metadata": {
        "id": "GzB9_4WAMHo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c3c6f3-874d-420e-b2e2-041d318df1dd"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264960\n",
            "8\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vol_train_mean = volu.mean()\n",
        "vol_train_sd = volu.std()\n",
        "\n",
        "np.save(\"vol_train_mean.npy\", vol_train_mean)\n",
        "np.save(\"vol_train_sd.npy\", vol_train_sd)"
      ],
      "metadata": {
        "id": "Ps0EegrBDUHr"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct features and dataset"
      ],
      "metadata": {
        "id": "jDjRe7xoJdZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_features(log_pr_df, vol_df):\n",
        "    df = log_pr_df.copy()\n",
        "    # ema21 = log_pr_df.ewm(span=21, min_periods=5, adjust=False).mean().fillna(1)\n",
        "    # ema35 = log_pr_df.ewm(span=35, min_periods=10, adjust=False).mean().fillna(1)\n",
        "    # ema80 = log_pr_df.ewm(span=80, min_periods=20, adjust=False).mean().fillna(1)\n",
        "    # ema250 = log_pr_df.ewm(span=250, min_periods=30, adjust=False).mean().fillna(1)\n",
        "\n",
        "    # ema12 = log_pr_df.ewm(span=12, min_periods=12, adjust=False).mean().fillna(1)\n",
        "    # ema26 = log_pr_df.ewm(span=26, min_periods=26, adjust=False).mean().fillna(1)\n",
        "    # macd = ema12 - ema26\n",
        "    # macd_s = macd.ewm(span=9, min_periods=9, adjust=False).mean().fillna(1)\n",
        "    # macd_h = macd - macd_s\n",
        "    \n",
        "    # ## simple moving averaga and rollinger bands\n",
        "    # sma = log_pr_df.rolling(window = 60).mean()\n",
        "    # sma = sma.fillna(sma.iloc[60])\n",
        "    # rstd = log_pr_df.rolling(window = 60).std()\n",
        "    # rstd = rstd.fillna(rstd.iloc[60])\n",
        "    # upper_bollinger = sma - 2*rstd\n",
        "    # lower_bollinger = sma +2*rstd\n",
        "\n",
        "    # # relative strength index\n",
        "    # delta = log_pr_df.diff(periods = 1).fillna(0)\n",
        "    # delta_neg = delta.clip(upper=0).abs()\n",
        "    # delta_pos = delta.clip(lower=0)\n",
        "    # down = delta_neg.ewm(span=60, adjust=False).mean().fillna(1)\n",
        "    # up = delta_pos.ewm(span=60, adjust=False).mean().fillna(1)\n",
        "    # rs = (up/down).replace([np.inf, np.nan],1)\n",
        "    # rsi = 100- 100/(1+rs)\n",
        "\n",
        "    #standardized volume\n",
        "    vol_standardized = (vol_df - vol_train_mean)/vol_train_sd\n",
        "    # df = pd.concat([ema80, vol_standardized, lower_bollinger, upper_bollinger, rsi, macd], axis=1)\n",
        "    df = pd.concat([df, vol_standardized], axis=1)\n",
        "    return df"
      ],
      "metadata": {
        "id": "eRn3ihxa2ENe"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "yrHfzn9Zxwt2"
      },
      "outputs": [],
      "source": [
        "def construct_dataset(window_size, features, log_prices, classification= False):\n",
        "    \"\"\"\n",
        "    window: look-back window size for constructing X (in minutes)\n",
        "    \"\"\"\n",
        "    window_dt = datetime.timedelta(minutes=window_size)\n",
        "    predict_dt = datetime.timedelta(minutes=30)\n",
        "\n",
        "    window_X = []\n",
        "    window_y = []\n",
        "\n",
        "    for t in features.index[window_size:-window_size:10]: # compute the predictions every 10 minutes\n",
        "      window_X.append(features.loc[(t - window_dt):t])\n",
        "      if classification:\n",
        "        window_y.append(np.sign(log_prices.loc[t + predict_dt] - log_prices.loc[t])) #changed to classification\n",
        "      else:\n",
        "        window_y.append(log_prices.loc[t + predict_dt] - log_prices.loc[t])\n",
        "        \n",
        "    return np.array(window_X), np.array(window_y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset_by_asset(dataset_train):\n",
        "  NUM_ASSETS = 10\n",
        "  return [dataset_train[..., asset::NUM_ASSETS] for asset in range(NUM_ASSETS)]"
      ],
      "metadata": {
        "id": "ohmDUTY2S7cr"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_local_features(asset_window):\n",
        "  \"\"\"\n",
        "  construct features for *single windowed asset*\n",
        "\n",
        "  NOTE: for adding new features, see asset_window[...,0] for np.std\n",
        "  Here, 0 is the feature that is being pulled out (corresponds to the index from global_feature construction)\n",
        "  from which you can do whatever transforms you want\n",
        "  \"\"\"\n",
        "\n",
        "  # print(\"A\", asset_window.shape)\n",
        "  # local_features = [asset_window.reshape(asset_window.shape[0], -1)] # flattens *per* training example\n",
        "  # local_features.append(np.std(asset_window[...,0], axis=-1, keepdims=True))\n",
        "  poly = PolynomialFeatures(2)\n",
        "\n",
        "  neg_back_30_for = -(asset_window[...,0][:, -1] - asset_window[...,0][:, -30]).reshape(-1, 1)\n",
        "  neg_back_60_for = -(asset_window[...,0][:, -1] - asset_window[...,0][:, -60]).reshape(-1, 1)\n",
        "  neg_back_30_60_for = -(asset_window[...,0][:, -30] - asset_window[...,0][:, -60]).reshape(-1, 1)\n",
        "  neg_back_30_for_sq = (np.power(neg_back_30_for, 2))\n",
        "  neg_back_30_for_four = (np.power(neg_back_30_for, 4))\n",
        "  neg_back_30_60_for_sq = np.power(neg_back_30_60_for, 2)\n",
        "  neg_back_30_60_for_four = np.power(neg_back_30_60_for, 4)\n",
        "  volt_pr = np.std(asset_window[...,0], axis=-1, keepdims=True)\n",
        "  mean_vol = np.mean(asset_window[...,1], axis=-1, keepdims=True)\n",
        "  local_features = [neg_back_30_for, neg_back_30_for_sq, neg_back_30_for_four, neg_back_30_60_for, neg_back_30_60_for_sq, neg_back_30_60_for_four, volt_pr, mean_vol]\n",
        "  test = poly.fit_transform(np.hstack(local_features))\n",
        "\n",
        "\n",
        "  # local_features.append(np.var(asset_window[...,1], axis=-1, keepdims=True))\n",
        "  return test"
      ],
      "metadata": {
        "id": "XDKaL6zYCYqg"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models_split(dataset_train_by_asset, y_train, train_idxs, model_types):\n",
        "  NUM_ASSETS = 10\n",
        "  return [(model_types[asset].fit(construct_local_features(dataset_train_by_asset[asset]), \n",
        "                           y_train[:, asset]), asset) \n",
        "          for asset in range(NUM_ASSETS) if asset in train_idxs]"
      ],
      "metadata": {
        "id": "RD0WrSmOVznw"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_clean(A,B):\n",
        "  window_size = 241\n",
        "\n",
        "  input_features = (construct_features(A, B)).iloc[-window_size:] # only retain tail window\n",
        "  predictions = -(input_features.iloc[-1, :10] - input_features.iloc[-30,:10]).values # init baseline\n",
        "  input_features = input_features.values\n",
        "  split_features_by_asset = split_dataset_by_asset(input_features)\n",
        "  for model, asset_idx in split_models:\n",
        "    # print(asset_idx)\n",
        "    # if model is not None:\n",
        "      # expand_dims is used to align dimensions from \"batching\" used in training\n",
        "    predictions[asset_idx] = model.predict(construct_local_features(np.expand_dims(split_features_by_asset[asset_idx], axis=0)))\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "YKsnMlpodBgc"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = []\n",
        "for k in range(len(log_pr_kfolds)):\n",
        "  print(f\"Performing {k} fold...\")\n",
        "  log_pr_train, log_pr_test = log_pr_kfolds[k]\n",
        "  volu_train, volu_test  = volu_kfolds[k]\n",
        "  features = construct_features(log_pr_train, volu_train)\n",
        "\n",
        "  print(log_pr_train.shape)\n",
        "  print(log_pr_test.shape)\n",
        "\n",
        "  window_size = 240 # in minutes\n",
        "  X_train, y_train = construct_dataset(window_size, features, log_pr_train, classification= False)\n",
        "  X_train_by_asset = split_dataset_by_asset(X_train)\n",
        "  # 0, 3, 5 this gave 0.022\n",
        "  print(X_train.shape)\n",
        "  # print(X_train_by_asset.shape)\n",
        "  models = []\n",
        "  for _ in range(10):\n",
        "    models.append(RidgeCV(alphas=[1e-10,1e-2,1e-1,1,1e1,1e2]))\n",
        "  models[0] = HuberRegressor()\n",
        "  models[2] = HuberRegressor()\n",
        "  # models[1] = LinearRegression()\n",
        "  # models[4] = HuberRegressor(alpha=1e-10, epsilon=1, max_iter=500)\n",
        "  split_models = train_models_split(X_train_by_asset, y_train, [0, 2, 3, 5, 7, 9], models)\n",
        "\n",
        "  def get_model_corr(test_log_pr, test_volu):\n",
        "    w = 240\n",
        "    t0 = time.time()\n",
        "    dt = datetime.timedelta(days=1)\n",
        "    r_hat = pd.DataFrame(index=test_log_pr.index[w::10], columns=np.arange(10), dtype=np.float64)\n",
        "    print(r_hat.shape)\n",
        "    for t in test_log_pr.index[w::10]: # compute the predictions every 10 minutes\n",
        "        r_hat.loc[t, :] = get_r_hat_clean(test_log_pr.loc[(t - dt):t], test_volu.loc[(t - dt):t])\n",
        "    t_used = time.time() - t0\n",
        "    \n",
        "    r_fwd = (test_log_pr.shift(-30) - test_log_pr).iloc[w::10].rename(columns={f\"input_df_{i}\": i for i in range(10)})\n",
        "    r_fwd.corrwith(r_hat)\n",
        "    \n",
        "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final \"ignore_rows\" rows are NaNs. \n",
        "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
        "    return np.corrcoef(r_fwd_all, r_hat_all)[0, 1]\n",
        "\n",
        "  t0 = time.time()\n",
        "  ans = get_model_corr(log_pr_test, volu_test)\n",
        "  # ans = get_model_corr(log_pr, volu)\n",
        "  tracker.append(ans)\n",
        "  t_used = time.time() - t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdOLoczdlLOJ",
        "outputId": "f775e22e-ef7a-4627-edf1-a7071f6050d6"
      },
      "execution_count": 307,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing 0 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 1 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 2 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 3 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 4 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 5 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 6 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n",
            "Performing 7 fold...\n",
            "(50000, 10)\n",
            "(25000, 10)\n",
            "(4952, 241, 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2476, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tracker)\n",
        "print(np.average(tracker))\n",
        "print(np.median(tracker))\n",
        "print(np.max(tracker))\n",
        "print(np.min(tracker))\n",
        "print(np.average(np.abs(tracker)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drxRkjz7wMvR",
        "outputId": "481ee817-fa0f-4955-8e2f-ac1fbdd6bebb"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.020358001732029077, 0.08317406471208891, 0.061437585986611296, 0.05451884840435659, 0.05541174741325585, 0.04290244830534461, 0.02585217836658326, 0.022692690869687577]\n",
            "0.04579344572374465\n",
            "0.0487106483548506\n",
            "0.08317406471208891\n",
            "0.020358001732029077\n",
            "0.04579344572374465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, idx in split_models:\n",
        "  print(idx, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkjwTCY9WZUj",
        "outputId": "7024b34e-f809-4ac6-c873-fbbd4254be19"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 HuberRegressor()\n",
            "2 HuberRegressor()\n",
            "3 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "5 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "7 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "9 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train model on the full dataset"
      ],
      "metadata": {
        "id": "biH35k64IjnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = construct_features(log_pr, volu)\n",
        "\n",
        "window_size = 240 # in minutes\n",
        "X_train, y_train = construct_dataset(window_size, features, log_pr)\n",
        "X_train_by_asset = split_dataset_by_asset(X_train)\n",
        "# 0, 3, 5 this gave 0.022\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok-HEnzPIjLQ",
        "outputId": "5bd926c8-a3e7-4ca4-e9b7-70ec0b2d26ed"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26448, 241, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for _ in range(10):\n",
        "  models.append(RidgeCV(alphas=[1e-10,1e-2,1e-1,1,1e1,1e2]))\n",
        "models[0] = HuberRegressor()\n",
        "models[2] = HuberRegressor()\n",
        "# models[3] = HuberRegressor()\n",
        "# models[4] = HuberRegressor(alpha=1e-10, epsilon=1, max_iter=500)\n",
        "split_models = train_models_split(X_train_by_asset, y_train, [0, 2, 3, 5, 7, 9], models)\n",
        "dump(split_models, 'split_models.joblib') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NPaS9nlJ8a6",
        "outputId": "12597d75-e5bc-48c9-e6da-43fff6d86132"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['split_models.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splt_models = load('split_models.joblib') \n",
        "for model, asset_idx in split_models:\n",
        "  print(asset_idx)\n",
        "  print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3FXaV0QKEZW",
        "outputId": "0e32372f-3329-484a-f544-90cafb0e7b47"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "HuberRegressor()\n",
            "2\n",
            "HuberRegressor()\n",
            "3\n",
            "RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "5\n",
            "RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "7\n",
            "RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "9\n",
            "RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py to submit"
      ],
      "metadata": {
        "id": "p_HVo5MMSLWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_models = load('split_models.joblib')\n",
        "for model, asset_idx in split_models:\n",
        "    print(asset_idx, model) \n",
        "\n",
        "vol_train_mean = np.load(\"vol_train_mean.npy\")\n",
        "vol_train_sd = np.load(\"vol_train_sd.npy\")\n",
        "\n",
        "print(vol_train_mean)\n",
        "print(vol_train_sd)\n",
        "\n",
        "def get_r_hat(A, B): \n",
        "    \"\"\"\n",
        "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
        "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
        "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
        "    \"\"\"\n",
        "\n",
        "    def split_dataset_by_asset(dataset_train):\n",
        "        NUM_ASSETS = 10\n",
        "        return [dataset_train[..., asset::NUM_ASSETS] for asset in range(NUM_ASSETS)]\n",
        "\n",
        "    def construct_local_features(asset_window):\n",
        "        \"\"\"\n",
        "        construct features for *single windowed asset*\n",
        "\n",
        "        NOTE: for adding new features, see asset_window[...,0] for np.std\n",
        "        Here, 0 is the feature that is being pulled out (corresponds to the index from global_feature construction)\n",
        "        from which you can do whatever transforms you want\n",
        "        \"\"\"\n",
        "        poly = PolynomialFeatures(2)\n",
        "\n",
        "        neg_back_30_for = -(asset_window[...,0][:, -1] - asset_window[...,0][:, -30]).reshape(-1, 1)\n",
        "        neg_back_60_for = -(asset_window[...,0][:, -1] - asset_window[...,0][:, -60]).reshape(-1, 1)\n",
        "        neg_back_30_60_for = -(asset_window[...,0][:, -30] - asset_window[...,0][:, -60]).reshape(-1, 1)\n",
        "        neg_back_30_for_sq = (np.power(neg_back_30_for, 2))\n",
        "        neg_back_30_for_four = (np.power(neg_back_30_for, 4))\n",
        "        neg_back_30_for_six = (np.power(neg_back_30_for, 6))\n",
        "        neg_back_30_60_for_sq = np.power(neg_back_30_60_for, 2)\n",
        "        volt_pr = np.std(asset_window[...,0], axis=-1, keepdims=True)\n",
        "        mean_vol = np.mean(asset_window[...,1], axis=-1, keepdims=True)\n",
        "        local_features = [neg_back_30_for, neg_back_30_for_sq, neg_back_30_for_four, neg_back_30_60_for, volt_pr, mean_vol]\n",
        "        test = poly.fit_transform(np.hstack(local_features))\n",
        "        return test\n",
        "    def construct_features(log_pr_df, vol_df):\n",
        "        df = log_pr_df.copy()\n",
        "        # ema21 = log_pr_df.ewm(span=21, min_periods=5, adjust=False).mean().fillna(1)\n",
        "        # ema35 = log_pr_df.ewm(span=35, min_periods=10, adjust=False).mean().fillna(1)\n",
        "        vol_standardized = (vol_df - vol_train_mean)/vol_train_sd\n",
        "        # df = pd.concat([ema80, vol_standardized, lower_bollinger, upper_bollinger, rsi, macd], axis=1)\n",
        "        df = pd.concat([df, vol_standardized], axis=1)\n",
        "        return df\n",
        "\n",
        "    window_size = 241\n",
        "\n",
        "    input_features = (construct_features(A, B)).iloc[-window_size:] # only retain tail window\n",
        "    predictions = -(input_features.iloc[-1, :10] - input_features.iloc[-30,:10]).values # init baseline\n",
        "    input_features = input_features.values\n",
        "    split_features_by_asset = split_dataset_by_asset(input_features)\n",
        "    for model, asset_idx in split_models:\n",
        "      predictions[asset_idx] = model.predict(construct_local_features(np.expand_dims(split_features_by_asset[asset_idx], axis=0)))\n",
        "    return predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3xgQwrASNrw",
        "outputId": "614fcda0-2953-4270-fd8a-bedc4d416ede"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 HuberRegressor()\n",
            "2 HuberRegressor()\n",
            "3 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "5 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "7 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "9 RidgeCV(alphas=array([1.e-10, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02]))\n",
            "[  484138.67022853   738718.81738375   510729.47329492   582643.22591117\n",
            "   108312.27895552   753195.09578735   845414.71939756   324828.85258177\n",
            "  5221814.18025898 11535247.55395589]\n",
            "[  694563.75001621  1690094.07999115   798914.10623349   935308.69164905\n",
            "   199913.78094482  1353733.60609396  1466886.32927583   468701.2081696\n",
            "  7447967.156716   17660948.77560515]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test for speed"
      ],
      "metadata": {
        "id": "9FN2_cehSYAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_corr(log_pr, volu_pr, get_r_hat):\n",
        "    t0 = time.time()\n",
        "    dt = datetime.timedelta(days=1)\n",
        "    r_hat = pd.DataFrame(index=log_pr.index[120::10], columns=np.arange(10), dtype=np.float64)\n",
        "    print(r_hat.shape)\n",
        "    for t in log_pr.index[120::10]: # compute the predictions every 10 minutes\n",
        "        r_hat.loc[t, :] = get_r_hat(log_pr.loc[(t - dt):t], volu_pr.loc[(t - dt):t])\n",
        "    t_used = time.time() - t0\n",
        "    \n",
        "    print(t_used)\n",
        "get_model_corr(log_pr, volu, get_r_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVlmYrOASZlo",
        "outputId": "1cf9c615-6db0-4755-9c5e-9657c875c93d"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26484, 10)\n",
            "137.89684629440308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering Analysis"
      ],
      "metadata": {
        "id": "mKUmT18dGBqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "std_pr = log_pr.std(axis=0).values.reshape(-1, 1)\n",
        "r_fwd = (log_pr.shift(-30) - log_pr).iloc[30::10]\n",
        "mean_pr = r_fwd.mean(axis=0).values.reshape(-1, 1)\n",
        "std_fwd_pr =r_fwd.std(axis=0).values.reshape(-1, 1)\n",
        "\n",
        "volu_log = np.log(volu + 1)\n",
        "v_fwd = (volu_log.shift(-30) - volu_log).iloc[30::10]\n",
        "std_vol = volu_log.std(axis=0).values.reshape(-1, 1)\n",
        "mean_vol = v_fwd.mean(axis=0).values.reshape(-1, 1)\n",
        "std_fwd_vol = v_fwd.std(axis=0).values.reshape(-1, 1)\n",
        "\n",
        "features = np.concatenate((mean_pr, std_pr, std_vol, mean_vol, std_fwd_vol, std_fwd_pr), axis = 1)\n",
        "print(features.shape)\n",
        "\n",
        "distortions = []\n",
        "K = range(1,10)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k)\n",
        "    kmeanModel.fit(features)\n",
        "    distortions.append(kmeanModel.inertia_)"
      ],
      "metadata": {
        "id": "m-nYo3qSGBPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6MGAJ1OALqkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeanModel = KMeans(n_clusters=3)\n",
        "kmeanModel.fit(features)\n",
        "print(kmeanModel.labels_)"
      ],
      "metadata": {
        "id": "P1t9qN83M1_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "clustering = AgglomerativeClustering().fit(features)\n",
        "clustering.labels_"
      ],
      "metadata": {
        "id": "ev7qIyZ6M8hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "hTSIJGH7JqHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_pr.plot(figsize=(12, 8))"
      ],
      "metadata": {
        "id": "hA3imcLD9o2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "volu.plot(figsize=(12, 8))"
      ],
      "metadata": {
        "id": "TZaE-FpL8USC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "6dR1OlnV0jLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "4_v9yGy_Jiec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTMs"
      ],
      "metadata": {
        "id": "ZS5aTovmSQTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable \n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "izfnP2X2ZrLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensors = Variable(torch.Tensor(X_train).to(device))\n",
        "y_train_tensors = Variable(torch.Tensor(y_train).to(device))\n",
        "\n",
        "print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)"
      ],
      "metadata": {
        "id": "Kt55VF6CYXx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM1(nn.Module):\n",
        "    def __init__(self, output_size, input_size, hidden_size, num_layers):\n",
        "        super(LSTM1, self).__init__()\n",
        "        self.output_size = output_size #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc = nn.Linear(32, output_size) #fully connected last layer\n",
        "    \n",
        "    def forward(self,x):\n",
        "        output, (hn, cn) = self.lstm(x) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.fc1(hn)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc(out) #Final Output\n",
        "        return out"
      ],
      "metadata": {
        "id": "CQHjSqpJSPxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100 #1000 epochs\n",
        "learning_rate = 0.001 #0.001 lr\n",
        "\n",
        "input_size = X_train.shape[-1] #number of features\n",
        "print(input_size)\n",
        "hidden_size = 128 #number of features in hidden state\n",
        "num_layers = 1 #number of stacked lstm layers\n",
        "\n",
        "output_size = 10 #number of output classes\n",
        "\n",
        "lstm1 = LSTM1(output_size, input_size, hidden_size, num_layers) \n",
        "lstm1 = lstm1.to(device)\n",
        "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
        "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)\n",
        "\n",
        "print(X_train_tensors.shape)"
      ],
      "metadata": {
        "id": "9ndy3ZdgV94S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  outputs = lstm1.forward(X_train_tensors) \n",
        "  optimizer.zero_grad() \n",
        " \n",
        "  # obtain the loss function\n",
        "  loss = criterion(outputs, y_train_tensors)\n",
        " \n",
        "  loss.backward() \n",
        " \n",
        "  optimizer.step() #improve from loss, i.e backprop\n",
        "  if epoch % 2 == 0:\n",
        "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
      ],
      "metadata": {
        "id": "08j9WqBBWsSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm1 = lstm1.to(torch.device(\"cpu\"))\n",
        "torch.save(lstm1.state_dict(), \"/content/lstm_model.pth\")"
      ],
      "metadata": {
        "id": "yzZ-1cPQc436"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.expand_dims(log_pr.iloc[-31:, :].to_numpy(), 0).shape)"
      ],
      "metadata": {
        "id": "uZvySfrKfYEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm1.load_state_dict(torch.load(\"/content/lstm_model.pth\"))\n",
        "lstm1.eval()\n",
        "lstm1 = lstm1.to(torch.device(\"cpu\"))"
      ],
      "metadata": {
        "id": "blDR3M_w4Kiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GBoost"
      ],
      "metadata": {
        "id": "vcDi4DmPggul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = lgb.LGBMRegressor()\n",
        "print(X_train.shape)\n",
        "X_train_asset0 = np.array(X_train[:, :, 0])\n",
        "X_train_asset2 = np.array(X_train[:, :, 2])\n",
        "X_train_asset3 = np.array(X_train[:, :, 3])\n",
        "X_train_asset1 = np.array(X_train[:, :, 1])\n",
        "\n",
        "y_train_asset0 = np.array(y_train[:, 0])\n",
        "y_train_asset2 = np.array(y_train[:, 2])\n",
        "y_train_asset3 = np.array(y_train[:, 3])\n",
        "y_train_asset1 = np.array(y_train[:, 1])\n",
        "\n",
        "model_2 = lgb.LGBMRegressor()\n",
        "model_3 = lgb.LGBMRegressor()\n",
        "model_1 = lgb.LGBMRegressor()\n",
        "\n",
        "\n",
        "model_0.fit(X_train_asset0, y_train_asset0)\n",
        "model_2.fit(X_train_asset2, y_train_asset2)\n",
        "model_3.fit(X_train_asset3, y_train_asset3)\n",
        "model_1.fit(X_train_asset1, y_train_asset1)\n",
        "\n",
        "model_0.booster_.save_model('model_0.txt')\n",
        "model_2.booster_.save_model('model_2.txt')\n",
        "model_3.booster_.save_model('model_3.txt')\n",
        "model_1.booster_.save_model('model_1.txt')"
      ],
      "metadata": {
        "id": "q9hoezbohP2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = lgb.Booster(model_file='model_0.txt')\n",
        "model_2 = lgb.Booster(model_file='model_2.txt')\n",
        "model_3 = lgb.Booster(model_file='model_3.txt')\n",
        "model_1 = lgb.Booster(model_file='model_1.txt')"
      ],
      "metadata": {
        "id": "MMX0vKg-a-va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GBoost + Ridge"
      ],
      "metadata": {
        "id": "Ms0aBYa1_wnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = lgb.LGBMRegressor()\n",
        "model_01 = Ridge()\n",
        "model_35 = Ridge()\n",
        "\n",
        "#X_train_asset2 = np.array(X_train[:, :, 2])\n",
        "X_train_asset2 = np.concatenate([X_train[:, :, 2], X_train[:, :, 12], np.array([np.std(X_train[:, :, 2], axis = 1)]).T], axis =1)\n",
        "X_train_asset01 = np.concatenate((X_train[:, :, 0], X_train[:, :, 1]), axis=1)\n",
        "\n",
        "X_train_asset35 = np.concatenate((X_train[:, :, 3], X_train[:, :, 5]), axis=1)\n",
        "\n",
        "y_train_asset2 = np.array(y_train[:, 2])\n",
        "y_train_asset01 = np.concatenate((y_train[:, 0:1], y_train[:, 1:2]), axis=1)\n",
        "y_train_asset35 = np.concatenate((y_train[:, 3:4], y_train[:, 5:6]), axis=1)\n",
        "\n",
        "model_2.fit(X_train_asset2, y_train_asset2)\n",
        "model_01.fit(X_train_asset01, y_train_asset01)\n",
        "model_35.fit(X_train_asset35, y_train_asset35)\n",
        "\n"
      ],
      "metadata": {
        "id": "HeF86fu3_wZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ridge+Huber\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U6eo1J0-lmpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "_57x3AyjSK6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models(X_train, y_train):\n",
        "  X_train_asset0 = np.concatenate([X_train[:, :, 0], X_train[:, :, 10], np.array([np.std(X_train[:, :, 0], axis = 1)]).T], axis =1)\n",
        "  X_train_asset1 = np.concatenate([X_train[:, :, 1], X_train[:, :, 11], np.array([np.std(X_train[:, :, 1], axis = 1)]).T], axis =1)\n",
        "  X_train_asset2 = np.concatenate([X_train[:, :, 2], X_train[:, :, 12], np.array([np.std(X_train[:, :, 2], axis = 1)]).T], axis =1)\n",
        "  X_train_asset3 = np.concatenate([X_train[:, :, 3], X_train[:, :, 13], np.array([np.std(X_train[:, :, 3], axis = 1)]).T], axis =1)\n",
        "  X_train_asset4 = np.concatenate([X_train[:, :, 4], X_train[:, :, 14], np.array([np.std(X_train[:, :, 4], axis = 1)]).T], axis =1)\n",
        "  X_train_asset5 = np.concatenate([X_train[:, :, 5], X_train[:, :, 15], np.array([np.std(X_train[:, :, 5], axis = 1)]).T], axis =1)\n",
        "  X_train_asset6 = np.array(X_train[:, :, 6])\n",
        "  X_train_asset7 = np.array(X_train[:, :, 7])\n",
        "  X_train_asset8 = np.array(X_train[:, :, 8])\n",
        "  X_train_asset9 = np.array(X_train[:, :, 9])\n",
        "\n",
        "  y_train_asset0 = np.array(y_train[:, 0])\n",
        "  y_train_asset1 = np.array(y_train[:, 1])\n",
        "  y_train_asset2 = np.array(y_train[:, 2])\n",
        "  y_train_asset3 = np.array(y_train[:, 3])\n",
        "  y_train_asset4 = np.array(y_train[:, 4])\n",
        "  y_train_asset5 = np.array(y_train[:, 5])\n",
        "  y_train_asset6 = np.array(y_train[:, 6])\n",
        "  y_train_asset7 = np.array(y_train[:, 7])\n",
        "  y_train_asset8 = np.array(y_train[:, 8])\n",
        "  y_train_asset9 = np.array(y_train[:, 9])\n",
        "\n",
        "  model_0 = Ridge()\n",
        "  model_1 = Ridge()\n",
        "  model_2 = Ridge()\n",
        "  model_3 = Ridge()\n",
        "  model_4 = Ridge()\n",
        "  model_5 = Ridge()\n",
        "\n",
        "\n",
        "  print(X_train_asset0.shape)\n",
        "  model_0.fit(X_train_asset0, y_train_asset0)\n",
        "  model_1.fit(X_train_asset1, y_train_asset1)\n",
        "  model_2.fit(X_train_asset2, y_train_asset2)\n",
        "  model_3.fit(X_train_asset3, y_train_asset3)\n",
        "  #model_4.fit(X_train_asset4, y_train_asset4)\n",
        "  model_5.fit(X_train_asset5, y_train_asset5)\n",
        "  #model_6.fit(X_train_asset6, y_train_asset6)\n",
        "  ##model_7.fit(X_train_asset7, y_train_asset7)\n",
        "  #model_8.fit(X_train_asset8, y_train_asset8)\n",
        "  #model_9.fit(X_train_asset9, y_train_asset9)\n",
        "  return model_0, model_1, model_2, model_3, model_5\n"
      ],
      "metadata": {
        "id": "ndINNU0wlwvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "PqnE4kzzhSsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mprkHj3xwt6"
      },
      "outputs": [],
      "source": [
        "# Use the negative 30-minutes backward log-returns to predict the 30-minutes forward log-returns\n",
        "#predict the log price, and then do correlation\n",
        "def get_r_hat_baseline(A, B):\n",
        "    return -(A.iloc[-1] - A.iloc[-30]).values "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_lstm(A, B):\n",
        "  input = np.expand_dims(construct_features(A, B).values, axis=0)\n",
        "  input = Variable(torch.Tensor(input))\n",
        "  pred = lstm1(input).detach().cpu().numpy()\n",
        "  return pred.squeeze()"
      ],
      "metadata": {
        "id": "u41btF83fFM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_complex(A, B):\n",
        "  w = 31\n",
        "  input = construct_features(A, B)\n",
        "  tmp = -(input.iloc[-1] - input.iloc[-30]).values\n",
        "  asset_0_pred = model_0.predict(np.expand_dims(input.iloc[-w:, 0].values, axis=0)) \n",
        "  #asset_2_pred = model_2.predict(np.expand_dims(input.iloc[-w:, 2].values, axis=0))\n",
        "  asset_2_pred = model_2.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 2].values, input.iloc[-w:, 12].values, [np.std(input.iloc[-w:, 2].values)]]), axis=0))\n",
        "  asset_3_pred = model_3.predict(np.expand_dims(input.iloc[-w:, 3].values, axis=0))\n",
        "  asset_1_pred = model_1.predict(np.expand_dims(input.iloc[-w:, 1].values, axis=0))\n",
        "  tmp[0] = asset_0_pred[0]\n",
        "  tmp[2] = asset_2_pred[0]\n",
        "  tmp[3] = asset_3_pred[0]\n",
        "  tmp[1] = asset_1_pred[0]\n",
        "\n",
        "  return tmp"
      ],
      "metadata": {
        "id": "1tEScM3UotCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_more_features(A,B):\n",
        "  w = 61\n",
        "  input = construct_features(A, B)\n",
        "  tmp = -(input.iloc[-1, :10] - input.iloc[-30,:10]).values\n",
        "  asset_0_pred = model_0.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 0].values, input.iloc[-w:, 10].values, [np.std(input.iloc[-w:, 0].values)]]), axis=0))\n",
        "  asset_2_pred = model_2.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 2].values, input.iloc[-w:, 12].values, [np.std(input.iloc[-w:, 2].values)]]), axis=0))\n",
        "  asset_3_pred = model_3.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 3].values, input.iloc[-w:, 13].values, [np.std(input.iloc[-w:, 3].values)]]), axis=0))\n",
        "  asset_1_pred = model_1.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 1].values, input.iloc[-w:, 11].values, [np.std(input.iloc[-w:, 1].values)]]), axis=0))\n",
        "  #asset_4_pred = model_4.predict(np.expand_dims(np.concatenate([input.iloc[-61:, 4].values, input.iloc[-61:, 14].values, [np.std(input.iloc[-61:, 4].values)]]), axis=0)) \n",
        "  asset_5_pred = model_5.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 5].values, input.iloc[-w:, 15].values, [np.std(input.iloc[-w:, 5].values)]]), axis=0))\n",
        "  #asset_5_pred = model_5.predict(np.expand_dims(input.iloc[-61:, 5].values, axis=0))\n",
        "  #asset_6_pred = model_6.predict(np.expand_dims(input.iloc[-61:, 6].values, axis=0))\n",
        "  #asset_7_pred = model_7.predict(np.expand_dims(input.iloc[-61:, 7].values, axis=0))\n",
        "  #asset_8_pred = model_8.predict(np.expand_dims(input.iloc[-61:, 8].values, axis=0))\n",
        "  #asset_9_pred = model_9.predict(np.expand_dims(input.iloc[-61:, 9].values, axis=0))\n",
        "  tmp[0] = asset_0_pred[0]\n",
        "  tmp[2] = asset_2_pred[0]\n",
        "  tmp[3] = asset_3_pred[0]\n",
        "  tmp[1] = asset_1_pred[0]\n",
        "  #tmp[4] = asset_4_pred[0]\n",
        "  tmp[5] = asset_5_pred[0]\n",
        "  #tmp[6] = asset_6_pred[0]\n",
        "  #tmp[7] = asset_7_pred[0]\n",
        "  #tmp[8] = asset_8_pred[0]\n",
        "  #tmp[9] = asset_9_pred[0]\n",
        "\n",
        "\n",
        "  return tmp"
      ],
      "metadata": {
        "id": "Hzduuz7BlZw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_simple(A, B):\n",
        "  input = construct_features(A, B)\n",
        "  return -(input.iloc[-1] - input.iloc[-30]).values "
      ],
      "metadata": {
        "id": "_vGoGX5jot55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r_hat_ridge_boost(A, B):\n",
        "  input = construct_features(A, B)\n",
        "  tmp = -(input.iloc[-1] - input.iloc[-30]).values\n",
        "  #asset_2_pred = model_2.predict(np.expand_dims(input.iloc[-61:, 2].values, axis=0))\n",
        "  asset_2_pred = model_2.predict(np.expand_dims(np.concatenate([input.iloc[-61:, 2].values, input.iloc[-61:, 12].values, [np.std(input.iloc[-61:, 2].values)]]), axis=0))\n",
        "  input_01 = np.concatenate((input.iloc[-61:, 0:1].values, input.iloc[-61:, 1:2].values), axis=1).reshape(1, -1)\n",
        "  asset_01_pred = np.squeeze(model_01.predict(input_01))\n",
        "\n",
        "  input_35 = np.concatenate((input.iloc[-61:, 3:4].values, input.iloc[-61:, 5:6].values), axis=1).reshape(1, -1)\n",
        "  asset_35_pred = np.squeeze(model_35.predict(input_35))\n",
        "\n",
        "  tmp[2] = asset_2_pred[0]\n",
        "  tmp[0] = asset_01_pred[0]\n",
        "  tmp[1] = asset_01_pred[1]\n",
        "  tmp[3] = asset_35_pred[0]\n",
        "  tmp[5] = asset_35_pred[1]\n",
        "\n",
        "\n",
        "  return tmp\n"
      ],
      "metadata": {
        "id": "8Vc2T3S8Be-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMc8AuzExwt6"
      },
      "outputs": [],
      "source": [
        "# An example of get_r_hat\n",
        "\n",
        "ACTIVE_R_HAT = \"more_features\"\n",
        "\n",
        "r_hat_implementations = {\n",
        "    \"baseline\": get_r_hat_baseline, \n",
        "    \"lstm\": get_r_hat_lstm,\n",
        "    \"simple\":get_r_hat_simple,\n",
        "    \"complex\": get_r_hat_complex, \n",
        "    \"ridge_boost\": get_r_hat_ridge_boost,\n",
        "    \"more_features\": get_r_hat_more_features\n",
        "}\n",
        "\n",
        "def get_r_hat(A, B): \n",
        "    \"\"\"\n",
        "        A: 1440-by-10 dataframe of log prices with columns log_pr_0, ... , log_pr_9\n",
        "        B: 1440-by-10 dataframe of trading volumes with columns volu_0, ... , volu_9    \n",
        "        return: a numpy array of length 10, corresponding to the predictions for the forward 30-minutes returns of assets 0, 1, 2, ..., 9\n",
        "    \"\"\"\n",
        "    return r_hat_implementations[ACTIVE_R_HAT](A, B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN6LhZA_xwt7"
      },
      "outputs": [],
      "source": [
        "def get_model_corr(test_log_pr, test_volu):\n",
        "    t0 = time.time()\n",
        "    dt = datetime.timedelta(days=1)\n",
        "    r_hat = pd.DataFrame(index=test_log_pr.index[1440::10], columns=np.arange(10), dtype=np.float64)\n",
        "    for t in test_log_pr.index[1440::10]: # compute the predictions every 10 minutes\n",
        "        r_hat.loc[t, :] = get_r_hat(test_log_pr.loc[(t - dt):t], test_volu.loc[(t - dt):t])\n",
        "    t_used = time.time() - t0\n",
        "    \n",
        "    r_fwd = (test_log_pr.shift(-30) - test_log_pr).iloc[1440::10].rename(columns={f\"input_df_{i}\": i for i in range(10)})\n",
        "    r_fwd.corrwith(r_hat)\n",
        "    \n",
        "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final \"ignore_rows\" rows are NaNs. \n",
        "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
        "    return np.corrcoef(r_fwd_all, r_hat_all)[0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = []\n",
        "for k in range(len(log_pr_kfolds)):\n",
        "  print(k)\n",
        "  log_pr_train, log_pr_test = log_pr_kfolds[k]\n",
        "  volu_train, volu_test  = log_pr_kfolds[k]\n",
        "\n",
        "  features = construct_features(log_pr_train, volu_train)\n",
        "  window_size = 60 # in minutes\n",
        "  X_train, y_train = construct_dataset(window_size, features, log_pr_train)\n",
        "\n",
        "  print(X_train.shape)\n",
        "\n",
        "  model_0, model_1, model_2, model_3, model_5 = train_models(X_train, y_train)\n",
        "\n",
        "  def get_r_hat(A,B):\n",
        "    w = 61\n",
        "    input = construct_features(A, B)\n",
        "    tmp = -(input.iloc[-1, :10] - input.iloc[-30,:10]).values\n",
        "    asset_0_pred = model_0.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 0].values, input.iloc[-w:, 10].values, [np.std(input.iloc[-w:, 0].values)]]), axis=0))\n",
        "    asset_2_pred = model_2.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 2].values, input.iloc[-w:, 12].values, [np.std(input.iloc[-w:, 2].values)]]), axis=0))\n",
        "    asset_3_pred = model_3.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 3].values, input.iloc[-w:, 13].values, [np.std(input.iloc[-w:, 3].values)]]), axis=0))\n",
        "    asset_1_pred = model_1.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 1].values, input.iloc[-w:, 11].values, [np.std(input.iloc[-w:, 1].values)]]), axis=0))\n",
        "    asset_5_pred = model_5.predict(np.expand_dims(np.concatenate([input.iloc[-w:, 5].values, input.iloc[-w:, 15].values, [np.std(input.iloc[-w:, 5].values)]]), axis=0))\n",
        "    tmp[0] = asset_0_pred[0]\n",
        "    tmp[2] = asset_2_pred[0]\n",
        "    tmp[3] = asset_3_pred[0]\n",
        "    tmp[1] = asset_1_pred[0]\n",
        "    tmp[5] = asset_5_pred[0]\n",
        "\n",
        "    return tmp\n",
        "\n",
        "  def get_model_corr(test_log_pr, test_volu):\n",
        "    w = 60\n",
        "    t0 = time.time()\n",
        "    dt = datetime.timedelta(days=1)\n",
        "    r_hat = pd.DataFrame(index=test_log_pr.index[w::10], columns=np.arange(10), dtype=np.float64)\n",
        "    print(r_hat.shape)\n",
        "    for t in test_log_pr.index[w::10]: # compute the predictions every 10 minutes\n",
        "        r_hat.loc[t, :] = get_r_hat_clean(test_log_pr.loc[(t - dt):t], test_volu.loc[(t - dt):t])\n",
        "    t_used = time.time() - t0\n",
        "    \n",
        "    r_fwd = (test_log_pr.shift(-30) - test_log_pr).iloc[w::10].rename(columns={f\"input_df_{i}\": i for i in range(10)})\n",
        "    r_fwd.corrwith(r_hat)\n",
        "    \n",
        "    r_fwd_all = r_fwd.iloc[:-3].values.ravel() # the final \"ignore_rows\" rows are NaNs. \n",
        "    r_hat_all = r_hat.iloc[:-3].values.ravel()\n",
        "    return np.corrcoef(r_fwd_all, r_hat_all)[0, 1]\n",
        "\n",
        "  t0 = time.time()\n",
        "  ans = get_model_corr(log_pr_test, volu_test)\n",
        "  tracker.append(ans)\n",
        "  t_used = time.time() - t0\n"
      ],
      "metadata": {
        "id": "LXKZAmnjRUap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tracker)\n",
        "print(np.average(tracker))\n",
        "print(np.median(tracker))\n",
        "print(np.max(tracker))\n",
        "print(np.min(tracker))\n",
        "print(np.average(np.abs(tracker)))"
      ],
      "metadata": {
        "id": "xC69U6aoW8sg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mKUmT18dGBqn",
        "hTSIJGH7JqHt"
      ],
      "name": "Copy of stats_601_project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}